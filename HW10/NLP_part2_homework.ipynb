{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvgXC6YxSm12"
      },
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 1. Реализация Greedy_sampling и generate\n",
        "\n",
        "Greedy sampling – всегда выбираем токен с максимальным логитом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        if logits.dim() == 2:  # (batch_sz, vocab_sz)\n",
        "            token_id = torch.argmax(logits, dim=-1)[0]\n",
        "        elif logits.dim() == 1:  # (vocab_sz)\n",
        "            token_id = torch.argmax(logits)\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,  # количество новых токенов\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3,\n",
        "    ) -> str:\n",
        "        self.model.eval()\n",
        "\n",
        "        input_promt_tok = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        generated = input_promt_tok\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                outputs = self.model(input_ids=generated)\n",
        "\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "                next_token_id = self.greedy_sampling(next_token_logits)\n",
        "\n",
        "                next_token_tensor = torch.tensor(\n",
        "                    [[next_token_id]], device=self.device, dtype=generated.dtype\n",
        "                )\n",
        "                generated = torch.cat([generated, next_token_tensor], dim=1)\n",
        "\n",
        "                # ранняя остановка по EOS\n",
        "                if next_token_id == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine learning is a very powerful tool for learning about the world around us. It's a tool that can help us understand the world around us, and it's a tool that can help us understand the world around us.\n",
            "\n",
            "The world is changing. We're\n"
          ]
        }
      ],
      "source": [
        "model = Model(\"gpt2\")\n",
        "print(model.generate(\"Machine learning is\", max_length=50, strategy=\"greedy\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 2. Реализация метода random sampling и поддержка его в generate\n",
        "\n",
        "Random sampling – случайный выбор токена по softmax распределению."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        if logits.dim() == 2:  # (batch_sz, vocab_sz)\n",
        "            token_id = torch.argmax(logits, dim=-1)[0]\n",
        "        elif logits.dim() == 1:  # (vocab_sz)\n",
        "            token_id = torch.argmax(logits)\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "\n",
        "        if logits.dim() == 2:  # (batch_sz,vocab_sz)\n",
        "            logits = logits[0]\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)  # (vocab_sz)\n",
        "        token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,  # количество новых токенов\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3,\n",
        "    ) -> str:\n",
        "        self.model.eval()\n",
        "\n",
        "        input_promt_tok = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        generated = input_promt_tok\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                outputs = self.model(input_ids=generated)\n",
        "\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                if strategy == \"greedy\":\n",
        "                    next_token_id = self.greedy_sampling(next_token_logits)\n",
        "                elif strategy == \"random\":\n",
        "                    next_token_id = self.random_sampling(next_token_logits)\n",
        "\n",
        "                next_token_tensor = torch.tensor(\n",
        "                    [[next_token_id]], device=self.device, dtype=generated.dtype\n",
        "                )\n",
        "                generated = torch.cat([generated, next_token_tensor], dim=1)\n",
        "\n",
        "                if next_token_id == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GREEDY:\n",
            "Machine learning is a very powerful tool for learning about the world around us. It's a tool that can help us understand the world around us, and it's a\n",
            "\n",
            "RANDOM:\n",
            "Machine learning is solid and intelligent work, and in this more egalitarian Zurich, the project will once again bring weirder roles for cognitive design to play inside ACM\n",
            "\n",
            "GREEDY:\n",
            "Machine learning is a very powerful tool for learning about the world around us. It's a tool that can help us understand the world around us, and it's a\n",
            "\n",
            "RANDOM:\n",
            "Machine learning is an American invention in several respects. It involves exploration of datasets for global North American smog recorders and consumers can have accuracy were it to adjust their\n"
          ]
        }
      ],
      "source": [
        "model = Model(\"gpt2\")\n",
        "\n",
        "print(\"GREEDY:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"greedy\"))\n",
        "\n",
        "print(\"\\nRANDOM:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"random\"))\n",
        "\n",
        "print(\"\\nGREEDY:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"greedy\"))\n",
        "\n",
        "print(\"\\nRANDOM:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"random\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 3. Реализация метода beam_search_generate и поддержка его в generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beam search – на каждом шаге:\n",
        "1. Для варианта считаем логиты следующего токена;\n",
        "2. Идем по всем веткам на фиксир. глубину и считаем вероятность по ветке;\n",
        "3. Выбираем лучшую ветку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        if logits.dim() == 2: # (batch_sz, vocab_sz)\n",
        "            token_id = torch.argmax(logits, dim=-1)[0]\n",
        "        elif logits.dim() == 1: # (vocab_sz)\n",
        "            token_id = torch.argmax(logits)\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        \n",
        "        if logits.dim() == 2: #(batch_sz,vocab_sz)\n",
        "            logits = logits[0]\n",
        "        \n",
        "        probs = F.softmax(logits, dim=-1) #(vocab_sz)\n",
        "        token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int # количество гипотез на шаге\n",
        "    ) -> str:\n",
        "        self.model.eval()\n",
        "        \n",
        "        input_promt_tok = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        eos_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        beams = [(input_promt_tok, 0.0)]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "  \n",
        "                all_input_batch = torch.cat([seq for (seq, _) in beams], dim=0)  # (B, seq_len)\n",
        "\n",
        "                outputs = self.model(input_ids=all_input_batch)\n",
        "          \n",
        "                next_logits = outputs.logits[:, -1, :] # логит последнего токена\n",
        "\n",
        "                log_probs = F.log_softmax(next_logits, dim=-1)  # (B, vocab)\n",
        "                vocab_size = log_probs.size(-1)\n",
        "\n",
        "                beam_scores = torch.tensor([score for (_, score) in beams],\n",
        "device=self.device)\n",
        "\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs  # (B, vocab)\n",
        "\n",
        "                total_scores_row = total_scores.view(-1)  # (B * vocab)\n",
        "                topk = min(num_beams, total_scores_row.size(0))\n",
        "                top_scores, top_indices = torch.topk(total_scores_row, k=topk)\n",
        "\n",
        "                new_beams = []\n",
        "                for score, idx in zip(top_scores, top_indices):\n",
        "  \n",
        "                    beam_idx = (idx // vocab_size).item()\n",
        "                    token_id = (idx % vocab_size).item()\n",
        "\n",
        "                    seq, _ = beams[beam_idx]\n",
        "\n",
        "                    token_tensor = torch.tensor([[token_id]], device=self.device, dtype=seq.dtype)\n",
        "\n",
        "                    new_seq = torch.cat([seq, token_tensor], dim=1)\n",
        "\n",
        "                    new_beams.append((new_seq, float(score.item())))\n",
        "\n",
        "                beams = new_beams\n",
        "\n",
        "                if all(seq[0, -1].item() == eos_id for (seq, _) in beams):\n",
        "                    break\n",
        "                \n",
        "        best_seq, _ = max(beams, key=lambda x: x[1])\n",
        "        return self.tokenizer.decode(best_seq[0], skip_special_tokens=True)\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50, # количество новых токенов\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        self.model.eval()\n",
        "\n",
        "        if strategy == \"beam\":\n",
        "            return self._beam_search_generate(prompt=prompt,max_length=max_length, num_beams=num_beams)\n",
        "\n",
        "        input_promt_tok= self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        generated = input_promt_tok\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                outputs = self.model(input_ids=generated)\n",
        "            \n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                if strategy == \"greedy\":\n",
        "                    next_token_id = self.greedy_sampling(next_token_logits)\n",
        "                elif strategy == \"random\":\n",
        "                    next_token_id = self.random_sampling(next_token_logits)\n",
        "\n",
        "                next_token_tensor = torch.tensor(\n",
        "                    [[next_token_id]],\n",
        "                    device=self.device,\n",
        "                    dtype=generated.dtype\n",
        "                )\n",
        "                generated = torch.cat([generated, next_token_tensor], dim=1)\n",
        "\n",
        "                if next_token_id == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEAM (num_beams=3):\n",
            "Machine learning is a great way to learn about the world around you. It's a great way to learn about the world around you. It's a great way to\n",
            "\n",
            "BEAM (num_beams=6):\n",
            "Machine learning is one of the most promising areas of research in the field of artificial intelligence.\n",
            "\n",
            "In a paper published in the Proceedings of the National Academy of Sciences\n",
            "\n",
            "BEAM (num_beams=9):\n",
            "Machine learning is one of the most promising areas of research in the field of artificial intelligence.\n",
            "\n",
            "In a paper published in the Proceedings of the National Academy of Sciences\n"
          ]
        }
      ],
      "source": [
        "model = Model(\"gpt2\")\n",
        "\n",
        "print(\"BEAM (num_beams=3):\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"beam\", num_beams=3))\n",
        "\n",
        "print(\"\\nBEAM (num_beams=6):\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"beam\", num_beams=5))\n",
        "\n",
        "print(\"\\nBEAM (num_beams=9):\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"beam\", num_beams=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 4. Реализация методов apply_top_p, apply_top_k, apply_temperature и поддержка их в generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Apply_temperature – масштабирование распределения логитов;\n",
        "2. Apply_top_k – оставляем только top_k токенов с максимальными логитами;\n",
        "3. Apply_top_p – берем токены, отсортирвоанные по убыванию вер., суммируем из вероятности, пока сумма не превысит порог. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        if logits.dim() == 2: # (batch_sz, vocab_sz)\n",
        "            token_id = torch.argmax(logits, dim=-1)[0]\n",
        "        elif logits.dim() == 1: # (vocab_sz)\n",
        "            token_id = torch.argmax(logits)\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        \n",
        "        if logits.dim() == 2: #(batch_sz,vocab_sz)\n",
        "            logits = logits[0]\n",
        "        \n",
        "        probs = F.softmax(logits, dim=-1) #(vocab_sz)\n",
        "        token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return int(token_id.item())\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int # количество гипотез на шаге\n",
        "    ) -> str:\n",
        "        self.model.eval()\n",
        "        \n",
        "        input_promt_tok = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        eos_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        beams = [(input_promt_tok, 0.0)]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "  \n",
        "                all_input_batch = torch.cat([seq for (seq, _) in beams], dim=0)  # (B, seq_len)\n",
        "\n",
        "                outputs = self.model(input_ids=all_input_batch)\n",
        "          \n",
        "                next_logits = outputs.logits[:, -1, :] # логит последнего токена\n",
        "\n",
        "                log_probs = F.log_softmax(next_logits, dim=-1)  # (B, vocab)\n",
        "                vocab_size = log_probs.size(-1)\n",
        "\n",
        "                beam_scores = torch.tensor([score for (_, score) in beams],\n",
        "device=self.device)\n",
        "\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs  # (B, vocab)\n",
        "\n",
        "                total_scores_row = total_scores.view(-1)  # (B * vocab)\n",
        "                topk = min(num_beams, total_scores_row.size(0))\n",
        "                top_scores, top_indices = torch.topk(total_scores_row, k=topk)\n",
        "\n",
        "                new_beams = []\n",
        "                for score, idx in zip(top_scores, top_indices):\n",
        "  \n",
        "                    beam_idx = (idx // vocab_size).item()\n",
        "                    token_id = (idx % vocab_size).item()\n",
        "\n",
        "                    seq, _ = beams[beam_idx]\n",
        "\n",
        "                    token_tensor = torch.tensor([[token_id]], device=self.device, dtype=seq.dtype)\n",
        "\n",
        "                    new_seq = torch.cat([seq, token_tensor], dim=1)\n",
        "\n",
        "                    new_beams.append((new_seq, float(score.item())))\n",
        "\n",
        "                beams = new_beams\n",
        "\n",
        "                if all(seq[0, -1].item() == eos_id for (seq, _) in beams):\n",
        "                    break\n",
        "                \n",
        "        best_seq, _ = max(beams, key=lambda x: x[1])\n",
        "        return self.tokenizer.decode(best_seq[0], skip_special_tokens=True)\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "\n",
        "        if top_p >= 1.0:\n",
        "          return logits\n",
        "        \n",
        "        single = False\n",
        "        if logits.dim() == 1:\n",
        "            logits_bv = logits.unsqueeze(0)\n",
        "            single = True\n",
        "        elif logits.dim() == 2:\n",
        "            logits_bv = logits\n",
        "\n",
        "        ver = F.softmax(logits_bv, dim=-1)\n",
        "        sorted_ver, sorted_idx = ver.sort(dim=-1, descending=True)\n",
        "\n",
        "        cumsum = sorted_ver.cumsum(dim=-1)\n",
        "\n",
        "        keep_len = (cumsum < top_p).sum(dim=-1) + 1 # +1 — первый токен за top_p\n",
        "\n",
        "        V = ver.size(-1)\n",
        "        arange = torch.arange(V, device=ver.device).unsqueeze(0).expand_as(sorted_ver)\n",
        "        \n",
        "        keep_sorted_mask = arange < keep_len.unsqueeze(1)\n",
        "\n",
        "        keep_mask = torch.zeros_like(ver, dtype=torch.bool)\n",
        "        keep_mask.scatter_(1, sorted_idx, keep_sorted_mask)\n",
        "\n",
        "        neg_inf = logits_bv.new_full((), float(\"-inf\"))\n",
        "        filtered = torch.where(keep_mask, logits_bv, neg_inf)\n",
        "\n",
        "        return filtered.squeeze(0) if single else filtered\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int) -> torch.Tensor:\n",
        "\n",
        "        if top_k <= 0:\n",
        "            return logits\n",
        "\n",
        "        filtered_logits = logits.clone()\n",
        "\n",
        "        if filtered_logits.dim() == 1:\n",
        "            vocab_size = filtered_logits.size(0)\n",
        "            if top_k >= vocab_size:\n",
        "                return filtered_logits\n",
        "\n",
        "            topk_values, _ = torch.topk(filtered_logits, top_k)\n",
        "      \n",
        "            threshold = topk_values[-1]\n",
        "    \n",
        "            filtered_logits[filtered_logits < threshold] = float(\"-inf\")\n",
        "\n",
        "        elif filtered_logits.dim() == 2:\n",
        "            vocab_size = filtered_logits.size(1)\n",
        "            if top_k >= vocab_size:\n",
        "                return filtered_logits\n",
        "\n",
        "            topk_values, _ = torch.topk(filtered_logits, top_k, dim=-1)\n",
        "            thresholds = topk_values[:, -1].unsqueeze(-1)\n",
        "            mask = filtered_logits < thresholds\n",
        "            filtered_logits[mask] = float(\"-inf\")\n",
        "\n",
        "        return filtered_logits\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50, # количество новых токенов\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        self.model.eval()\n",
        "\n",
        "        if strategy == \"beam\":\n",
        "            return self._beam_search_generate(prompt=prompt,max_length=max_length, num_beams=num_beams)\n",
        "\n",
        "        input_promt_tok= self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        generated = input_promt_tok\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                outputs = self.model(input_ids=generated)\n",
        "            \n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                if strategy == \"greedy\":\n",
        "                    next_token_id = self.greedy_sampling(next_token_logits)\n",
        "                elif strategy == \"random\":\n",
        "                    \n",
        "                    modified_logits = next_token_logits\n",
        "                    \n",
        "                    if temperature != 1.0:\n",
        "                        modified_logits = self._apply_temperature(modified_logits, temperature)\n",
        "\n",
        "                    if top_k > 0:\n",
        "                        modified_logits = self._apply_top_k(modified_logits, top_k)\n",
        "\n",
        "                    if top_p < 1.0:\n",
        "                        modified_logits = self._apply_top_p(modified_logits, top_p)\n",
        "\n",
        "                    next_token_id = self.random_sampling(modified_logits)\n",
        "\n",
        "                next_token_tensor = torch.tensor(\n",
        "                    [[next_token_id]],\n",
        "                    device=self.device,\n",
        "                    dtype=generated.dtype\n",
        "                )\n",
        "                generated = torch.cat([generated, next_token_tensor], dim=1)\n",
        "\n",
        "                if next_token_id == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RANDOM, default:\n",
            "Machine learning is becoming increasingly sophisticated with each passing year.\n",
            "\n",
            "But does the technology give us confidence that simple, timeless algorithms can outperform demanding systems with significant UI\n",
            "\n",
            "RANDOM + temperature=1.5:\n",
            "Machine learning is taking the web further than ever before and Web applications are making real progress. In addition to recognition, researchers around the world are creating strong technology platforms for\n",
            "\n",
            "RANDOM + top_k=10:\n",
            "Machine learning is excellent science and science fiction narrative, but they miss the dark side. When it is a slush fund, female fans typically get what they came for\n",
            "\n",
            "RANDOM + top_p=0.9:\n",
            "Machine learning is a powerful approach when applied to cognitive plasticity in users. Follow along below for our sensuous treatise on this theme of learning via neural integration.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"RANDOM, default:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"random\"))\n",
        "\n",
        "print(\"\\nRANDOM + temperature=1.5:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"random\", temperature=1.5))\n",
        "\n",
        "print(\"\\nRANDOM + top_k=10:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"random\", top_k=10))\n",
        "\n",
        "print(\"\\nRANDOM + top_p=0.9:\")\n",
        "print(model.generate(\"Machine learning is\", max_length=30, strategy=\"random\", top_p=0.9))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
